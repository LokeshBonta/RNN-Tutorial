{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Seq2Seq Translation!\n",
    "\n",
    "As part of understanding LSTMs, we will build Seq2Seq translator for translating French to English. We do this 3 parts:\n",
    "\n",
    "***Order of Assignments***\n",
    "\n",
    "1. Encoder-Decoder Module for Seq2Seq\n",
    "2. Teacher Forcing Extension in Decoding Phase\n",
    "3. Attention Transfer for \"focusing\" on key phrases for translation..\n",
    "\n",
    "Lets get started with Encoder-Decoder Module..\n",
    "\n",
    "Just as in the previous assignments, please follow along the instructions provided in the cells/comments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Language Class\n",
    "\n",
    "Parameters:\n",
    "1. word2index dict\n",
    "2. index2word dict\n",
    "3. word2count dict - to filter-out infrequent words\n",
    "4. n_count variable - for counting number of unique words in the language\n",
    "\n",
    "Input:\n",
    "\n",
    "1. Name of the Language -> French/English etc\n",
    "\n",
    "Methods:\n",
    "\n",
    "1. addSentence using addWord\n",
    "2. addWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 2 #Including SOS and EOS token for each language\n",
    "    \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2count[word] = 1\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else :\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for String Preprocessing\n",
    "\n",
    "This module is already completed. This is basic string processing in python using regex and inbuilt utility functions\n",
    "\n",
    "1. Convert string to lowerCase\n",
    "2. remove leading or trailing spaces - use strip()\n",
    "3. Convert all UniCode to ASCII Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Note: The data is present in eng-fra.txt file. You need to load the data using the following steps\n",
    "\n",
    "1. Load data from file, removing spaces and splitting by new-line\n",
    "2. Each line has 2 parts:\n",
    "    1. The french text\n",
    "    2. Tab separation '\\t'\n",
    "    3. The English translation\n",
    "    \n",
    "3. Use normalize each of the strings and create pairs of sentences.\n",
    "4. define a language model\n",
    "5. Return input_lang,output_lang and the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1,lang2):\n",
    "    f = open('data/LSTM/eng-fra.txt', encoding='utf-8').read().strip().split(\"\\n\")\n",
    "    \n",
    "    pairs = [[normalizeString(p) for p in line.split('\\t')] for line in f]\n",
    "    \n",
    "    pairs = [list(reversed(p)) for p in pairs]\n",
    "    input_lang = Lang(lang2)\n",
    "    output_lang = Lang(lang1)\n",
    "    \n",
    "    return input_lang,output_lang,pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data by maximum sentence length, and basic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p): # boolean function. True -> keep the pair, False ->discard\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1,lang2):\n",
    "    input_lang,output_lang,pairs = readLangs(lang1,lang2)\n",
    "    pairs = filterPairs(pairs)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang,output_lang,pairs\n",
    "\n",
    "input_lang,output_lang,pairs = prepareData('eng','fra')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "<img src =\"images/lstm2.png\">\n",
    "\n",
    "We are going to simulate the network shown above. It consists of 2 stages. One is the encoding phase and other is the decoding phase..\n",
    "\n",
    "The Encode Module is going to follow the network structure given below. The purpose of encoder is 2 folds:\n",
    "\n",
    "1. Outputs some value for every word in the input sequence\n",
    "2. For every input word - output a vector and a hidden state.\n",
    "\n",
    "This hidden state is used as \"input\" for the word of the input sequence.\n",
    "\n",
    "<img src = \"images/lstm1.png\" >\n",
    "\n",
    "Here, we are using a differen type of RNNCell - called the GRU (Gated Recurrent Unit). It is a very popular variant of LSTM Cell.\n",
    "\n",
    "Please fill in the code below to build the encoder network. You need to fill in the RHS of each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,n_layers=1):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        \n",
    "        # define hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "        # define n_layers\n",
    "        self.n_layers = n_layers\n",
    "        # define an embedding from input_size to hidden_size\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        # gru from hidden to hidden (hidden of embedding to output-hidden)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "        \n",
    "    def forward(self,input,hidden):\n",
    "        # map embedding and reshape it to (1,1,-1) shape (seq_len,batch_Size and input_size)\n",
    "        embedded = self.embedding(input).view(1,1,-1) #seq_len,batch_size = 1\n",
    "        # save embedded in variable output\n",
    "        output = embedded\n",
    "        # for each gru layer - by default it is 1\n",
    "        for i in range(self.n_layers):\n",
    "            # output,hidden hold the return values from gru cell\n",
    "            output,hidden = self.gru(output,hidden)\n",
    "        \n",
    "        # return output and hidden after each forward pass\n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        # return hidden layer values - zeros of size (1,1,hidden_size)\n",
    "        result = Variable(torch.zeros(1,1,self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "<img src=\"images/lstm2.png\">\n",
    "\n",
    "Decoder is another RNN that takes encoders output vector/vectors and maps it to a sequence of words (translation)\n",
    "\n",
    "It takes the \"Context Vector\" from the encoder - the last output vector from the encoder module as its initial hidden state\n",
    "\n",
    "At every step, it is given an input token and a hidden state. Initial state is <SOS> token and the hidden state is the context vector from encoder (its last hidden state)\n",
    "\n",
    "<img src = \"images/lstm3.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,n_layers=1):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        # define hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "        # define n_layers\n",
    "        self.n_layers = n_layers\n",
    "        # define embedding taking output_size to hidden size\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size)\n",
    "        # gru cell to take hidden to hidden!\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size)\n",
    "        # linear from hidden to output size\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        # softmax classification - for prob of next word\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self,input,hidden):\n",
    "        # map input (which is encoder output) to (seq_len,batch_size,input_size) using embedding\n",
    "        output = self.embedding(input).view(1,1,-1) #seq_len,batch_size = 1,1\n",
    "        # for each of the gru layers\n",
    "        for i in range(self.n_layers):\n",
    "            # relu your output\n",
    "            output = F.relu(output)\n",
    "            # apply gru unit on output,hidden for next_level output,hidden\n",
    "            output,hidden = self.gru(output,hidden)\n",
    "        # apply softmax on the linear embedding of output[0]\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        # return output,hidden for next iteration\n",
    "        return output,hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training Data\n",
    "\n",
    "You have pairs [ french_Sentence, English_sentence]. We now need one-hot representation of each of these sentences, w.r.t to their own respective vocabularies. This module will focus on preparing you training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a sentence from language convert to a index-vector\n",
    "def indexesFromSentence(lang,sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "# convert the index-vector to tensor\n",
    "def variableFromSentence(lang,sentence):\n",
    "    indexes = indexesFromSentence(lang,sentence)\n",
    "    # add EOS flag\n",
    "    indexes.append(EOS_token)\n",
    "    # longTensor(index).view(-1,1) -> a column vector\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1,1))\n",
    "\n",
    "    return result\n",
    "\n",
    "def variableFromPair(pair):\n",
    "    # construct tensors for input and target for every pait\n",
    "    input_variable = variableFromSentence(input_lang,pair[0])\n",
    "    target_variable = variableFromSentence(output_lang,pair[1])\n",
    "    return input_variable,target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variableFromPair(pairs[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Now that we have defined our model, we need to train it to be able to perform seq2seq translation. The overview of the training process is as follows:\n",
    "\n",
    "1. Initialize hidden_layers with zeros\n",
    "2. Zero grad optimizers for both Encoder and Decoder RNN (Remember - there are 2 RNNs for each iteration)\n",
    "3. Define input and target lengths\n",
    "4. Define loss = 0\n",
    "\n",
    "## EncoderRNN Training\n",
    "\n",
    "1. For each word in the input sentence, pass it through the encoder\n",
    "2. Output of each time-step becomes the input for the next time-step\n",
    "\n",
    "\n",
    "<img src=\"images/lstm2.png\">\n",
    "\n",
    "Please complete the below module as per the steps given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable,target_variable,encoder,decoder,encoder_optimizer,decoder_optimizer, criterion):\n",
    "    # initialize encode hidden\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    # zero-out gradient for encoder and decoder optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # compute input and target length\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    # intialize loss to 0\n",
    "    loss = 0\n",
    "    # for each input index\n",
    "    for ei in range(input_length):\n",
    "        #pass it to the encoder with the hidden state\n",
    "        encoder_output,encoder_hidden = encoder(input_variable[ei],encoder_hidden)\n",
    "    \n",
    "    # append decoder input with sos-token\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    # decoder hidden will be encoder hidden\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # for each target index\n",
    "    for di in range(target_length):\n",
    "        # pass input and hidden through the decoder\n",
    "        decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "        # find index of maximun using decoder_output probabilities\n",
    "        topv,topi = decoder_output.data.topk(1)\n",
    "        next_index = topi[0][0]\n",
    "        # decoder input is the next variable Tensor\n",
    "        decoder_input = Variable(torch.LongTensor([[next_index]]))\n",
    "        # add to loss.. wait! dont backprop till the decoder computation is complete\n",
    "        loss += criterion(decoder_output,target_variable[di])\n",
    "        # if index is same as end of sentence then break!\n",
    "        if next_index==EOS_token:\n",
    "            break\n",
    "    \n",
    "    # back propagate loss\n",
    "    loss.backward()\n",
    "    # step optimizer\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    #return loss (normalized loss)\n",
    "    return loss.data[0]/target_length\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder,decoder,n_iter,print_every=1000,learning_rate = .01):\n",
    "    plot_loss = []\n",
    "    print_loss = 0\n",
    "    # define optimizers - encoder and decoder opts\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(),lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(),lr = learning_rate)\n",
    "    # generate random pairs..\n",
    "    training_set = [variableFromPair(random.choice(pairs)) for i in range(n_iter)]\n",
    "    # define NLLoss as loss criterion\n",
    "    criterion = nn.NLLLoss()\n",
    "    # for number of iterations\n",
    "    for i in range(1,n_iter+1):\n",
    "        # extract the input sentence\n",
    "        train_sentence = training_set[i-1]\n",
    "        # define input and target variable\n",
    "        input_variable = train_sentence[0]\n",
    "        target_variable = train_sentence[1]\n",
    "        # call train utilities\n",
    "        loss = train(input_variable,target_variable,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion)\n",
    "        # add loss\n",
    "        print_loss += loss\n",
    "        \n",
    "        if i%print_every==0:\n",
    "            avg_loss = print_loss/print_every\n",
    "            print (\"Iteration %d and Loss %f\" % (i,avg_loss))\n",
    "            print_loss = 0\n",
    "            plot_loss.append(avg_loss)\n",
    "    \n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # show loss plot\n",
    "    plt.plot(plot_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hidden_size\n",
    "hidden_size = 256\n",
    "# define encoder network\n",
    "encoder = EncoderRNN(input_lang.n_words,hidden_size)\n",
    "# define decoder network\n",
    "decoder = DecoderRNN(hidden_size,output_lang.n_words)\n",
    "# call trainIters\n",
    "trainIters(encoder,decoder,n_iter=75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluatePair(encoder,decoder,sentence,max_length = 10):\n",
    "    # index-tensor for input\n",
    "    input_variable = variableFromSentence(input_lang,sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    # define encoder hidden\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    # for each input-index\n",
    "    for ei in range(input_length):\n",
    "        # pass input index and encoder_hidden at each step for computing output and hidden\n",
    "        encoder_output,encoder_hidden = encoder(input_variable[ei],encoder_hidden)\n",
    "    # Decoder with sos input appended\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    # inital decoder hidden with encoder hidden\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #list of output words\n",
    "    decoder_words = []\n",
    "    # for max_len of output sequence\n",
    "    for di in range(max_length):\n",
    "        # pass decoder input with decoder hidden\n",
    "        decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "        # index of maximum becomes the predicted word\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        next_index = topi[0][0]\n",
    "        # if it is eos then break\n",
    "        if next_index==EOS_token:\n",
    "            decoder_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            # append output-word in list\n",
    "            decoder_words.append(output_lang.index2word[next_index])\n",
    "        # deocoder output at max-prob index becomes next input index\n",
    "        decoder_input = Variable(torch.LongTensor([[next_index]]))\n",
    "    # return words...\n",
    "    return decoder_words\n",
    "\n",
    "def evaluate(encoder,decoder,n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print (\"Input: \",pair[0])\n",
    "        print (\"Ground Truth: \",pair[1])\n",
    "        output_words = evaluatePair(encoder,decoder,pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print (\"Translation: \",output_sentence)\n",
    "        print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(encoder,decoder,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
