{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Character level text generation\n",
    "Welcome, in this lab-exercise, we will will building an RNN which generates text one character at a time! Hope you enjoy coding.\n",
    "\n",
    "## This is divided into 4 stages:\n",
    "\n",
    "1. Loading and Pre-Processing \n",
    "2. FeedForward computation\n",
    "3. Backpropagation through time (BPTT)\n",
    "4. Sentence generation\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Please follow along the comments/markdown provided to help you in completing your tasks. Feel free to reach-out to coordinators for help in coding.\n",
    "\n",
    "Lets get started.\n",
    "\n",
    "Sairam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyper parameters\n",
    "\n",
    "1. Sequence Length\n",
    "2. Size of Hidden Layer\n",
    "3. Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "\n",
    "# initialize hyper-parameters\n",
    "learning_rate = .003\n",
    "hidden_size = 100\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1\n",
    "\n",
    "1. Read input file using python 'open' functionality\n",
    "2. load file/sentences into 'data' variable (remove special chars!)\n",
    "3. build char_to_num and num_to_char dicts mappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars = 3873 and unique chars = 66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read input from file\n",
    "file = open('data/RNN/input_text.txt')\n",
    "data = file.read().strip()\n",
    "# define vocabulary\n",
    "vocab = set(data)\n",
    "\n",
    "# define data_size and vocab_size\n",
    "data_size,vocab_size = len(data),len(vocab)\n",
    "\n",
    "print(\"Number of chars = {} and unique chars = {}\\n\".format(data_size,vocab_size))\n",
    "\n",
    "# Define mapping from char to number and vice-versa\n",
    "\n",
    "char_to_num = {ch:i for i,ch in enumerate(vocab)}\n",
    "num_to_char = {i:ch for i,ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "\n",
    "1. Matrix U -> Input-to-Hidden\n",
    "2. Matrix W -> Hidden-to-Hidden Recurrence\n",
    "3. Matrix V -> Hidden-to-Output\n",
    "4. Matrix Bh -> Bias Input-to-Hidden (at hidden layer)\n",
    "5. Matrix Bo -> Bias Hidden-to-Output (at output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input to hidden\n",
    "U = np.random.randn(hidden_size,vocab_size) * .01\n",
    "# Hidden to hidden\n",
    "W = np.random.randn(hidden_size,hidden_size) * .01\n",
    "# Hidden to output\n",
    "V = np.random.randn(vocab_size,hidden_size) * .01\n",
    "# Bias for hidden\n",
    "Bh = np.zeros((hidden_size,1))\n",
    "# Bias for output\n",
    "Bo = np.zeros((vocab_size,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on untrained model parameters\n",
    "\n",
    "***Input ***: hidden_state, index of start_character and length of the sentence to be generated\n",
    "\n",
    "***Output***: Model generated sentence of specified length\n",
    "\n",
    "#### To Do\n",
    "\n",
    "1. Function to convert index to one-hot representation\n",
    "2. Define a softmax activation (for generating normalized log-probabilities)\n",
    "3. DO a forward pass and apply softmax for obtaining output probabilities\n",
    "4. Maximum from the output probability becomes input for the next iteration\n",
    "5. Convert max_prob_index position to obtain generated char (using num_to_char mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    val = np.exp(x)\n",
    "    return val/np.sum(val,axis=0)\n",
    "\n",
    "# one-hot encoding - take position as the argument\n",
    "def convert_index_to_one_hot(pos):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    x[pos]=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate One Character at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1b7mt]Uy/V+hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n"
     ]
    }
   ],
   "source": [
    "# to generate sequence - taking start char as input\n",
    "def generate(hidden_state,input_char_pos,trial_seq_length=200):\n",
    "   \n",
    "    # convert to one-hot\n",
    "    x = convert_index_to_one_hot(pos=input_char_pos)\n",
    "    gen_string = ''\n",
    "    \n",
    "    # generate sequence\n",
    "    for t in range(trial_seq_length):\n",
    "        # compute hidden_state\n",
    "        hidden_state = np.tanh(np.dot(U,x)+np.dot(W,hidden_state) + Bh)\n",
    "        # comput output probability\n",
    "        output = softmax(np.dot(V,hidden_state) + Bo)\n",
    "        # max_prob element is the next-character\n",
    "        next_x = np.argmax(output.ravel())\n",
    "        # use it as one-hot for next element\n",
    "        x = convert_index_to_one_hot(next_x)\n",
    "        # append to existing string\n",
    "        gen_string += num_to_char[next_x]\n",
    "    \n",
    "    print(gen_string)\n",
    "\n",
    "#start hidden state\n",
    "hidden_prev = np.zeros((hidden_size,1))\n",
    "#generate sentence - output will be random-chars which dont make sense!\n",
    "generate(hidden_prev,char_to_num['a'],trial_seq_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Input and Target Values\n",
    "\n",
    "We will write code to actually see how our inputs and targets look like. This will give a great perspective before going for the actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data [22, 4, 53, 14, 12, 57, 49, 57, 36, 4] \n",
      "Target data [4, 53, 14, 12, 57, 49, 57, 36, 4, 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "\n",
    "# obtain input_data - using sequence length: will be a list of int's [char index]\n",
    "input_data = [char_to_num[ch] for ch in data[pos:pos+seq_len]]\n",
    "# target data - similar to input, but right-shifted by 1 position\n",
    "target_data = [char_to_num[ch] for ch in data[pos+1:pos+seq_len+1]]\n",
    "\n",
    "print (\"Input data {} \\nTarget data {}\\n\".format(input_data,target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "Simple forward propagation with the following steps:\n",
    "\n",
    "1. Convert to one-hot representation\n",
    "2. Compute hidden layer values - np.tanh ( u*x + w*h_(t_1) ) for each time-step\n",
    "3. Compute output (from hidden to output) - Matrix \"V\" dot \"hidden\"\n",
    "4. output is softmax\n",
    "5. loss += -logloss\n",
    "\n",
    "<img src=\"images/rnn_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(input_data,target_data,hidden_prev):\n",
    "    # define dicts to hold values\n",
    "    x,h,y,o = {},{},{},{}\n",
    "    # store \"inital\" hidden state value\n",
    "    h[-1]=np.copy(hidden_prev)\n",
    "    # define loss\n",
    "    loss = 0\n",
    "    # for each input-timestep\n",
    "    \n",
    "    for t in range(len(input_data)):\n",
    "        # convert input[t] to one-hot representation\n",
    "        x[t]=convert_index_to_one_hot(input_data[t])\n",
    "        # tanh activation on hidden-state values\n",
    "        h[t]=np.tanh(np.dot(U,x[t])+np.dot(W,h[t-1])+Bh)\n",
    "        # dot-product to take from hidden-to-output\n",
    "        y[t]=np.dot(V,h[t])\n",
    "        # softmax on y to map to output-probabilities\n",
    "        o[t]=softmax(y[t])\n",
    "        # add to log-loss on target[t]\n",
    "        loss += -np.log(o[t][target_data[t],0])\n",
    "        \n",
    "    # return x,h,y,o,loss values for next-steps\n",
    "    return x,h,y,o,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation for each input\n",
    "\n",
    "Refer to hand-outs for equations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(input_data,target_data,x,h,o,clip_gradient=True):\n",
    "    # define graient matrices\n",
    "    dU,dW,dV = np.zeros_like(U),np.zeros_like(W),np.zeros_like(V)\n",
    "    dBh,dBo = np.zeros_like(Bh),np.zeros_like(Bo)\n",
    "    d_hidden_next = np.zeros_like(h)\n",
    "    # start from reverse\n",
    "    for t in reversed(range(len(input_data))):\n",
    "        # cache the data\n",
    "        dy = np.copy(o[t])\n",
    "        # dy = y -1\n",
    "        dy[target_data[t]] -= 1\n",
    "        # dot dy with hidden - remember to accumulate!\n",
    "        dV += np.dot(dy,h[t].T)\n",
    "        # dy is the gradient for output-bias\n",
    "        dBo += dy\n",
    "        # hidden gradient - dy.V + d_hidden_next (back-prop into hidden)\n",
    "        dh = np.dot(V.T,dy) + d_hidden_next\n",
    "        # tanh gradient * dh\n",
    "        dh_raw = np.array((1- h[t]*h[t]) * dh,dtype=np.float64)\n",
    "        # dh_raw for bias gradient\n",
    "        dBh += dh_raw\n",
    "        \n",
    "        # dot dh_raw, with x for dU\n",
    "        dU += np.dot(dh_raw,x[t].T)\n",
    "        # dot dh_raw with h for dW\n",
    "        dW += np.dot(dh_raw,h[t-1].T)\n",
    "        \n",
    "        # back_prop into hidden - time information.\n",
    "        d_hidden_next = np.dot(W.T,dh_raw)\n",
    "    \n",
    "    # Account for exploding gradient\n",
    "    if clip_gradient:\n",
    "        for param in [dU,dW,dV,dBh,dBo]:\n",
    "            # set range between -5 and 5\n",
    "            np.clip(param,-5,5,out=param)\n",
    "            \n",
    "    # return respective gradients\n",
    "    return dU,dW,dV,dBh,dBo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a single iteration: forward and backward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_one_iteration(input_data,target_data,hidden_prev):\n",
    "    \n",
    "    # Forward propgation\n",
    "    x,h,y,o,loss = forward_propagation(input_data,target_data,hidden_prev)\n",
    "    # Backward propagation\n",
    "    dU,dW,dV,dBh,dBo = back_propagation(input_data,target_data,x,h,o)\n",
    "    # return loss, gradients and hidden_state for next iteration\n",
    "    return loss,dU,dW,dV,dBh,dBo,h[len(input_data)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compute input_data and target_data -> will be indices\n",
    "2. do one-iteration of forward and backward\n",
    "3. Use the gradients and use Gradient Descent for online-update of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 41.896944603719525 and Iteration 0\n",
      "iiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "Loss: 27.052458964061096 and Iteration 5000\n",
      "tatatatatatatatatata\n",
      "\n",
      "Loss: 13.045024309853844 and Iteration 10000\n",
      "nput_data = np.dotat\n",
      "\n",
      "Loss: 21.595310197720874 and Iteration 15000\n",
      "dot(do_ntet_data,hid\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-23933d7f8c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# compute forward and backward prop and return gadients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_one_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Gradient Descent for the weight updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-519325ab0a18>\u001b[0m in \u001b[0;36mdo_one_iteration\u001b[0;34m(input_data, target_data, hidden_prev)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# return loss, gradients and hidden_state for next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdBo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d22b6c55cc4c>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(input_data, target_data, x, h, o, clip_gradient)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# dot dy with hidden - remember to accumulate!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdV\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# dy is the gradient for output-bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdBo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start position\n",
    "pos=0\n",
    "\n",
    "for iter_count in range(1000*1000):\n",
    "    # check for the bounds/end words\n",
    "    if pos+seq_len+1>len(data) or iter_count==0:\n",
    "        pos=0\n",
    "        hidden_prev = np.zeros((hidden_size,1))\n",
    "    \n",
    "    #compute input and output data indices\n",
    "    input_data = [char_to_num[ch] for ch in data[pos:pos+seq_len]]\n",
    "    target_data = [char_to_num[ch] for ch in data[pos+1:pos+seq_len+1]]\n",
    "    # compute forward and backward prop and return gadients\n",
    "    loss,dU,dW,dV,dBh,dBo,hidden_prev = do_one_iteration(input_data,target_data,hidden_prev)\n",
    "\n",
    "    # Gradient Descent for the weight updates\n",
    "    U -= learning_rate * dU\n",
    "    W -= learning_rate * dW\n",
    "    V -= learning_rate * dV\n",
    "    Bh -= learning_rate * dBh\n",
    "    Bo -= learning_rate * dBo    \n",
    "\n",
    "    # print loss\n",
    "    if iter_count%5000==0:\n",
    "        print(\"\\nLoss: {} and Iteration {}\".format(loss,iter_count))\n",
    "        generate(hidden_state=hidden_prev,input_char_pos=input_data[0],trial_seq_length=20)\n",
    "        \n",
    "    # remember to move pos by seq_len for each iteration!!\n",
    "    pos += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
